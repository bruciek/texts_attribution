{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-21T07:12:11.188193300Z",
     "start_time": "2023-07-21T07:12:08.763157Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean\n",
    "from feature_functions import *\n",
    "from natasha_func import get_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"dataset.pkl\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T07:12:12.458174200Z",
     "start_time": "2023-07-21T07:12:11.188193300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_feature(dataset, func):\n",
    "    feature = []\n",
    "    for book in dataset['books']:\n",
    "        dok = get_doc(book)\n",
    "        f = func(dok)\n",
    "        feature.append(f)\n",
    "    return feature"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T07:12:12.460291Z",
     "start_time": "2023-07-21T07:12:12.458174200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def dataset_target(dataset):\n",
    "    authors = list(set(dataset['authors']))\n",
    "    dict_authors = dict()\n",
    "    reverse_dict_authors = dict()\n",
    "    for index in range(len(authors)):\n",
    "        dict_authors[authors[index]] = index\n",
    "        reverse_dict_authors[index] = authors[index]\n",
    "    target = []\n",
    "    for author in dataset[\"authors\"]:\n",
    "        target += [dict_authors[author]]\n",
    "    return target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T07:12:12.460291Z",
     "start_time": "2023-07-21T07:12:12.459292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "target = dataset_target(dataset)\n",
    "dataset_for_ml = {\"targets\": target}\n",
    "df = pd.DataFrame(dataset_for_ml)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T07:12:12.460291Z",
     "start_time": "2023-07-21T07:12:12.459292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'inspect' has no attribute 'getargspec'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m functions \u001B[38;5;241m=\u001B[39m [frequency_of_i, frequency_of_comma, frequency_of_dot, frequency_of_exclamation_mark,frequency_of_question_mark, frequency_of_brackets, frequency_of_quotation_marks, frequency_of_numbers, number_of_unique_words, number_of_words_that_occur_once, frequency_of_longest_word, average_lenght_of_sentence_by_letters, frequency_of_words_in_initial_form, correlation_of_vowels_consonants, most_common_first_letter_in_words, most_common_first_letter_in_sents, frequency_of_a, frequency_of_o, frequency_of_ะต, frequency_of_n, frequency_of_t, average_length_of_help_words, len_of_most_common_word, average_lenght_of_nouns, average_lenght_of_adjectives, average_lenght_of_adverbs, average_lenght_of_verbs, correlation_of_short_long_words, correlation_of_long_medium_words, semicolon_freq, count_upper_words, freq_of_freq_word, freq_word_from_adjective, freq_word_from_noun, freq_word_from_verbs, freq_of_space, sentences_avg_len_symbols, capitalized_words_count_without_start_of_sentences, avg_syllable_per_noun, avg_syllable_per_verb, avg_syllable_per_adjective, avg_syllable_per_adverb, count_words_infinitive]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m functions:\n\u001B[1;32m----> 3\u001B[0m     feature \u001B[38;5;241m=\u001B[39m \u001B[43mget_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     df[func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m] \u001B[38;5;241m=\u001B[39m feature\n",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m, in \u001B[0;36mget_feature\u001B[1;34m(dataset, func)\u001B[0m\n\u001B[0;32m      2\u001B[0m feature \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m book \u001B[38;5;129;01min\u001B[39;00m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbooks\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m----> 4\u001B[0m     dok \u001B[38;5;241m=\u001B[39m \u001B[43mget_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbook\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     f \u001B[38;5;241m=\u001B[39m func(dok)\n\u001B[0;32m      6\u001B[0m     feature\u001B[38;5;241m.\u001B[39mappend(f)\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\ML\\natasha_func.py:44\u001B[0m, in \u001B[0;36mget_doc\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     41\u001B[0m tagger \u001B[38;5;241m=\u001B[39m NewsMorphTagger(emb)\n\u001B[0;32m     42\u001B[0m doc\u001B[38;5;241m.\u001B[39mtag_morph(tagger)\n\u001B[1;32m---> 44\u001B[0m morph_vocab \u001B[38;5;241m=\u001B[39m \u001B[43mMorphVocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc\u001B[38;5;241m.\u001B[39mtokens:\n\u001B[0;32m     46\u001B[0m     token\u001B[38;5;241m.\u001B[39mlemmatize(morph_vocab)\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\venv\\Lib\\site-packages\\natasha\\morph\\vocab.py:162\u001B[0m, in \u001B[0;36mMorphVocab.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 162\u001B[0m     \u001B[43mPymorphyAnalyzer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMorphForm\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:224\u001B[0m, in \u001B[0;36mMorphAnalyzer.__init__\u001B[1;34m(self, path, lang, result_type, units, probability_estimator_cls, char_substitutes)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_type_orig \u001B[38;5;241m=\u001B[39m result_type\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_char_substitutes(char_substitutes)\n\u001B[1;32m--> 224\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_units\u001B[49m\u001B[43m(\u001B[49m\u001B[43munits\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:235\u001B[0m, in \u001B[0;36mMorphAnalyzer._init_units\u001B[1;34m(self, units_unbound)\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m unit \u001B[38;5;129;01min\u001B[39;00m item[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 235\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_units\u001B[38;5;241m.\u001B[39mappend((\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_unit\u001B[49m\u001B[43m(\u001B[49m\u001B[43munit\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_units\u001B[38;5;241m.\u001B[39mappend((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_unit(item[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), \u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:246\u001B[0m, in \u001B[0;36mMorphAnalyzer._bound_unit\u001B[1;34m(self, unit)\u001B[0m\n\u001B[0;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_bound_unit\u001B[39m(\u001B[38;5;28mself\u001B[39m, unit):\n\u001B[1;32m--> 246\u001B[0m     unit \u001B[38;5;241m=\u001B[39m \u001B[43munit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m     unit\u001B[38;5;241m.\u001B[39minit(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m unit\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\venv\\Lib\\site-packages\\pymorphy2\\units\\base.py:35\u001B[0m, in \u001B[0;36mBaseAnalyzerUnit.clone\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclone\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_params\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\venv\\Lib\\site-packages\\pymorphy2\\units\\base.py:76\u001B[0m, in \u001B[0;36mBaseAnalyzerUnit._get_params\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_params\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     74\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Return a dict with the parameters for this analyzer unit. \"\"\"\u001B[39;00m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m---> 76\u001B[0m         (key, \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, \u001B[38;5;28;01mNone\u001B[39;00m)) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_param_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\venv\\Lib\\site-packages\\pymorphy2\\units\\base.py:70\u001B[0m, in \u001B[0;36mBaseAnalyzerUnit._get_param_names\u001B[1;34m(cls)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m:\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[1;32m---> 70\u001B[0m args, varargs, kw, default \u001B[38;5;241m=\u001B[39m \u001B[43minspect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetargspec\u001B[49m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m)\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(args[\u001B[38;5;241m1\u001B[39m:])\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'inspect' has no attribute 'getargspec'"
     ]
    }
   ],
   "source": [
    "functions = [frequency_of_i, frequency_of_comma, frequency_of_dot, frequency_of_exclamation_mark,frequency_of_question_mark, frequency_of_brackets, frequency_of_quotation_marks, frequency_of_numbers, number_of_unique_words, number_of_words_that_occur_once, frequency_of_longest_word, average_lenght_of_sentence_by_letters, frequency_of_words_in_initial_form, correlation_of_vowels_consonants, most_common_first_letter_in_words, most_common_first_letter_in_sents, frequency_of_a, frequency_of_o, frequency_of_ะต, frequency_of_n, frequency_of_t, average_length_of_help_words, len_of_most_common_word, average_lenght_of_nouns, average_lenght_of_adjectives, average_lenght_of_adverbs, average_lenght_of_verbs, correlation_of_short_long_words, correlation_of_long_medium_words, semicolon_freq, count_upper_words, freq_of_freq_word, freq_word_from_adjective, freq_word_from_noun, freq_word_from_verbs, freq_of_space, sentences_avg_len_symbols, capitalized_words_count_without_start_of_sentences, avg_syllable_per_noun, avg_syllable_per_verb, avg_syllable_per_adjective, avg_syllable_per_adverb, count_words_infinitive]\n",
    "for func in functions:\n",
    "    feature = get_feature(dataset, func)\n",
    "    df[func.__name__] = feature\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T07:12:18.774540800Z",
     "start_time": "2023-07-21T07:12:12.459292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('out.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validation(df, model):\n",
    "    accuracy = []\n",
    "    for shift in range(0, 10, 2):\n",
    "        authors_seen = []\n",
    "        test_ixs = []\n",
    "        for ixs in range(len(df)):\n",
    "            aut = df['targets'][ixs]\n",
    "            if authors_seen.count(aut) < 2:\n",
    "                authors_seen += [aut]\n",
    "                test_ixs += [ixs + shift]\n",
    "        train_ixs = list(set(range(len(df))) - set(test_ixs))\n",
    "        y_test = df.iloc[test_ixs]['targets']\n",
    "        x_test = df.iloc[test_ixs].drop(columns=[\"targets\"])\n",
    "        y_train = df.iloc[train_ixs]['targets']\n",
    "        x_train = df.iloc[train_ixs].drop(columns=[\"targets\"])\n",
    "\n",
    "        rfc = model\n",
    "        rfc.fit(x_train, y_train)\n",
    "        accuracy.append(accuracy_score(y_test, rfc.predict(x_test)))\n",
    "    return accuracy\n",
    "\n",
    "print( mean(validation(df, ExtraTreesClassifier())))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "validation(df, CatBoostClassifier())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
