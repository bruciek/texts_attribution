{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-20T04:34:19.963453500Z",
     "start_time": "2023-07-20T04:34:18.133599600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from freq_of_symbol import freq_of_symbol\n",
    "from freq_of_word import freq_of_word\n",
    "from number_of_unique_words import number_of_unique_words\n",
    "from number_of_words_that_occur_once import number_of_words_that_occur_once\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T04:34:19.976127Z",
     "start_time": "2023-07-20T04:34:19.963453500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Данные/Не обработанные/Белый/belyj_a-text_0013.fb2'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mget_words_by_Natasha\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_words\n\u001B[0;32m      3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\get_words_by_Natasha.py:16\u001B[0m\n\u001B[0;32m     13\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(without_strong[\u001B[38;5;241m2\u001B[39m:])\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[1;32m---> 16\u001B[0m \u001B[43mget_raw_text\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mДанные/Не обработанные/Белый/belyj_a-text_0013.fb2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# функция принимает текст (тот, что освобожден от пэшек) и выводит список слов (без знаков припенания)\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_words\u001B[39m(text):\n",
      "File \u001B[1;32m~\\PycharmProjects\\texts_attribution\\get_words_by_Natasha.py:3\u001B[0m, in \u001B[0;36mget_raw_text\u001B[1;34m(path_to_file)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_raw_text\u001B[39m(path_to_file):\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath_to_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m      4\u001B[0m         rtext \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m      5\u001B[0m         rtext_unicode \u001B[38;5;241m=\u001B[39m rtext\u001B[38;5;241m.\u001B[39mdecode(chardet\u001B[38;5;241m.\u001B[39mdetect(rtext)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Данные/Не обработанные/Белый/belyj_a-text_0013.fb2'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from get_words_by_Natasha import get_words\n",
    "dataset = pickle.load(open(\"dataset.pkl\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T04:34:20.786207500Z",
     "start_time": "2023-07-20T04:34:19.978209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "authors = list(set(dataset['authors']))\n",
    "dict_authors = dict()\n",
    "reverse_dict_authors = dict()\n",
    "for index in range(len(authors)):\n",
    "    dict_authors[authors[index]] = index\n",
    "    reverse_dict_authors[index] = authors[index]\n",
    "\n",
    "target = []\n",
    "for author in dataset[\"authors\"]:\n",
    "    target += [dict_authors[author]]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature1 = []\n",
    "feature2 = []\n",
    "for book in dataset['books']:\n",
    "    words = get_words(book.lower())\n",
    "    #сюда лемматизацию впихуем\n",
    "\n",
    "    #сюда по очереди все функции\n",
    "    feature1.append(freq_of_word(words, 'и'))\n",
    "\n",
    "feature1_silent_don = []\n",
    "for book_silent_don in dataset_silent_don['books']:\n",
    "    words_silen_don = get_words(book_silent_don.lower())\n",
    "    #сюда лемматизацию впихуем\n",
    "\n",
    "    #сюда по очереди все функции\n",
    "    feature1_silent_don.append(freq_of_word(words_silen_don, 'и'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_for_ml = {\"feature1\": feature1, \"targets\": target}\n",
    "df = pd.DataFrame(dataset_for_ml)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('out.zip', index=False)\n",
    "pd.read_csv('out.zip')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "authors_seen = []\n",
    "test_ixs = []\n",
    "train_ixs = []\n",
    "for ixs in range(len(df)):\n",
    "    aut = df['targets'][ixs]\n",
    "    if authors_seen.count(aut) < 2:\n",
    "        authors_seen += [aut]\n",
    "        test_ixs += [ixs]\n",
    "    else: train_ixs += [ixs]\n",
    "\n",
    "y_test = df.iloc[test_ixs]['targets']\n",
    "x_test = df.iloc[test_ixs].drop(columns=[\"targets\"])\n",
    "y_train = df.iloc[train_ixs]['targets']\n",
    "x_train = df.iloc[train_ixs].drop(columns=[\"targets\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "rfc = LogisticRegression()\n",
    "rfc.fit(x_train, y_train)\n",
    "accuracy_score(y_test, rfc.predict(x_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfc.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
